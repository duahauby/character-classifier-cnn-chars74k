{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duahauby/character-classifier-cnn-chars74k/blob/master/TS_handler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk transformers torchserve"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xY_pifwoXdk",
        "outputId": "23e48a8b-77a2-4aac-d0c4-2b7f0b2656ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting torchserve\n",
            "  Downloading torchserve-0.7.0-py3-none-any.whl (19.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.6 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 66.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from torchserve) (0.38.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from torchserve) (7.1.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from torchserve) (5.4.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from torchserve) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, torchserve\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 torchserve-0.7.0 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2a2r9WToF5s"
      },
      "outputs": [],
      "source": [
        "from abc import ABC\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import sys\n",
        "import torch\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "from unicodedata import normalize\n",
        "from ts.torch_handler.base_handler import BaseHandler\n",
        "from nltk.tokenize import word_tokenize\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class IntentV3Handler(BaseHandler, ABC):\n",
        "    \"\"\"\n",
        "    Intent v3 handler class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(IntentV3Handler, self).__init__()\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, ctx):\n",
        "        \"\"\"In this initialize function, the intent model is loaded.\n",
        "        Args:\n",
        "            ctx (context): It is a JSON Object containing information\n",
        "            pertaining to the model artefacts parameters.\n",
        "        \"\"\"\n",
        "        self.manifest = ctx.manifest\n",
        "        properties = ctx.system_properties\n",
        "        model_dir = properties.get(\"model_dir\")\n",
        "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
        "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
        "\n",
        "        self.device = torch.device(\n",
        "            \"cuda:\" + str(properties.get(\"gpu_id\"))\n",
        "            if torch.cuda.is_available() and properties.get(\"gpu_id\") is not None\n",
        "            else \"cpu\"\n",
        "        )\n",
        "\n",
        "        sys.path.append(os.path.join(model_dir, self.manifest[\"model\"][\"modelFile\"]))\n",
        "        from model import HSDModel\n",
        "        \n",
        "        if model_pt_path.endswith('.zip'):\n",
        "            with zipfile.ZipFile(model_pt_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(os.path.join(model_dir, \"pretrained\"))\n",
        "        else:\n",
        "            logger.warning(\"Model should be compressed in zip file format.\")\n",
        "        pretrained_path = os.path.join(model_dir, \"pretrained/weights\")\n",
        "        model_file = \"model.pth\"\n",
        "        model_file_path = os.path.join(pretrained_path, model_file)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n",
        "        state = torch.load(model_file_path, map_location=self.device)\n",
        "        self.mapping = state[\"id2label\"]\n",
        "        self.model = HSDModel(pretrained_path, num_classes=len(self.mapping), device=self.device)\n",
        "        self.model.to(self.device)\n",
        "        self.model.load_state_dict(state[\"weights\"])\n",
        "        self.model.eval()\n",
        "\n",
        "        self.sample_mapping2intent = {}\n",
        "        if 'sample_mapping2intent' in state:\n",
        "            self.sample_mapping2intent = state['sample_mapping2intent']\n",
        "\n",
        "        logger.info(\n",
        "            \"Pretrained intent model from path %s loaded successfully\", model_dir\n",
        "        )\n",
        "\n",
        "        self.initialized = True\n",
        "\n",
        "    def preprocess(self, requests):\n",
        "        \"\"\"Basic text preprocessing, based on the user's chocie of application mode.\n",
        "        Args:\n",
        "            requests (str): The Input data in the form of text is passed on to the preprocess\n",
        "            function.\n",
        "        Returns:\n",
        "            list : The preprocess function returns a list of Tensor for the size of the word tokens.\n",
        "        \"\"\"\n",
        "        logger.info(\"Received text: '%s'\", requests)\n",
        "        input_ids_batch = []\n",
        "        texts = []\n",
        "        for idx, data in enumerate(requests):\n",
        "            input_text = data.get(\"data\")\n",
        "            if input_text is None:\n",
        "                input_text = data.get(\"body\")\n",
        "            if isinstance(input_text, (bytes, bytearray)):\n",
        "                input_text = input_text.decode('utf-8')\n",
        "            logger.info(\"Received text: '%s'\", input_text)\n",
        "            input_text = clean_text(input_text.lower())\n",
        "            texts.append(input_text)\n",
        "            input_ids_batch.append(self.custom_tokenize(input_text))\n",
        "\n",
        "        max_length = max([len(idx) for idx in input_ids_batch])\n",
        "        padded_input_ids = np.ones((len(input_ids_batch), max_length), dtype=np.long)\n",
        "\n",
        "        for i, idx in enumerate(input_ids_batch):\n",
        "            padded_input_ids[i, :len(idx)] = idx\n",
        "\n",
        "        input_mask = np.ones(padded_input_ids.shape)\n",
        "        input_mask[padded_input_ids == 1] = 0\n",
        "\n",
        "        input_ids_batch = torch.tensor(padded_input_ids, device=self.device)\n",
        "        attention_mask_batch = torch.tensor(input_mask, device=self.device)\n",
        "\n",
        "        logger.info(\"Input shape is: '%s'\", input_ids_batch.shape)\n",
        "        return input_ids_batch, attention_mask_batch, texts\n",
        "\n",
        "    def custom_tokenize(self, text):\n",
        "        start_end_tokens = self.tokenizer.encode('')\n",
        "        if '[UNK]' in self.tokenizer.vocab:\n",
        "            unk_token = [self.tokenizer.vocab['[UNK]']]\n",
        "        elif '<unk>' in self.tokenizer.vocab:\n",
        "            unk_token = [self.tokenizer.vocab['<unk>']]\n",
        "        else:\n",
        "            unk_token = self.tokenizer.encode('<unk>')\n",
        "\n",
        "        sub_word_ids = []\n",
        "        for j, word in enumerate(text.split()):\n",
        "            if j != 0: word = ' ' + word\n",
        "            tokens = self.tokenizer.encode(word, add_special_tokens=False)\n",
        "            if len(tokens) > 3:\n",
        "                sub_word_ids.extend(unk_token)\n",
        "            else:\n",
        "                sub_word_ids.extend(tokens)\n",
        "        sub_word_ids = sub_word_ids[:self.model.config.max_position_embeddings - 4]\n",
        "        sub_word_ids = [start_end_tokens[0]] + sub_word_ids + [start_end_tokens[1]]\n",
        "\n",
        "        return sub_word_ids\n",
        "\n",
        "    def inference(self, input_batch):\n",
        "        \"\"\"Predict the class (or classes) of the received text using the\n",
        "        serialized transformers checkpoint.\n",
        "        Args:\n",
        "            input_batch (list): List of Text Tensors from the pre-process function is passed here\n",
        "        Returns:\n",
        "            list : It returns a list of the predicted value for the input text\n",
        "        \"\"\"\n",
        "        input_ids_batch, attention_mask_batch, texts = input_batch\n",
        "        logits = self.model(input_ids_batch, attention_mask_batch)\n",
        "        logits = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
        "        \n",
        "        inferences = []\n",
        "        for i, logit in enumerate(logits):\n",
        "            sort_idx = logit.argsort()[::-1]\n",
        "            labels = [self.mapping[idx] for idx in sort_idx]\n",
        "            scores = [float(logit[idx]) for idx in sort_idx]\n",
        "            if texts[i] in self.sample_mapping2intent:\n",
        "                labels[0] = self.sample_mapping2intent[texts[i]]\n",
        "                scores = [0] * len(scores)\n",
        "                scores[0] = 1.0\n",
        "            inferences.append(list(zip(labels, scores)))\n",
        "        return inferences\n",
        "\n",
        "    def postprocess(self, inference_output):\n",
        "        \"\"\"Post Process Function converts the predicted response into Torchserve readable format.\n",
        "        Args:\n",
        "            inference_output (list): It contains the predicted response of the input text.\n",
        "        Returns:\n",
        "            (list): Returns a list of the Predictions and Explanations.\n",
        "        \"\"\"\n",
        "        return inference_output\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = normalize('NFC', text)\n",
        "    text = text.lower()\n",
        "    text = \" \".join([w for w in word_tokenize(text) if w not in string.punctuation])\n",
        "\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MrRgqt0NoGm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BVs5qyCoHZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vf4JrZhOoHlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u4D8GzuDoHxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rHoxQsxQoH9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Zx4ffrCoIJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sgNxdNM3oIUS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}